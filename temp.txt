/etc/profile

* export CUDA_HOME=/usr/local/cuda-11.4
  export LD_LIBRARY_PATH=${CUDA_HOME}/lib64
  export PATH=${CUDA_HOME}/bin:${PATH}



1.调研AI防抖full-frame系列最新的两篇文献，基于warp的视频稳定一般分为两个步骤：1)motion estimation and smoothing; 2)frame warping。两篇文献的第一个步骤都基于已有的较好方法，创新点都在第二步，如何达到full-frame的warping。
Out-of-boundary View Synthesis（OVS）方法定义了Affinity Estination Network，用于外推光流边界，从而可以warp对应的pixel到外边界，补全原来需要crop的区域；文章代码未放出，且文中部分公式和方法阐述不清，需要继续跟进。
Full-frame Video Stabilization（FuSta）方法结合图像域和特征域，在混合域中融合，并利用类似残差的结构回复高频细节，克服了图像域融合多重影和伪影，特征域融合导致模糊的问题。耗时较长，单帧耗时在秒级别
2.上述文献的相关文献阅读，1）Learning video stabilization using optical flow 2）RAFT
1)Deep Video  Stabilization Using Adversarial Networks; 2)Contentaware unsupervised deep  homography estimation

1.结合目前对AI视频稳定的理解，回顾了之前粗略看过的文献（fullFrame, mesh_based)，加深理解；此类方法目前的瓶颈：1）需要使用未来帧；2）处理时间较长，基本都在100ms以上；3）参数量较大
2.调研了现有文献中的预校正方法，主流的预校正方法一般都是按照如下步骤进行：1）寻找匹配的特征点；2）计算光流；3）根据特征点光流拟合全局单应矩阵；
这种方案适用于纯粹依赖图像域特征的算法，如果后续要通过陀螺仪来进一步校正，此预校正不太合适。
陀螺仪只包含旋转信息，或许可以只提取全局单应的平移部分做预校正，待进一步调研和尝试。 
3.整理AI视频稳定的文献，ppt制作中；

下周计划：
1.研究和复现PWC-Net和DUT的开源代码
2.文献调研
2021-Depth-Aware Multi-Grid Deep Homography Estimation with Contextual Correlation
2018-Robust Gyroscope-Aided Camera Self-Calibration
3.继续整理ppt，准备交流

本周进展：
1.文献阅读2021-Depth-Aware Multi-Grid Deep Homography Estimation with Contextual Correlation；文中根据已有的单目深度估计模型预测depth map，根据相邻grid的深度差异决定是否需要做shape-preserve;想法很好，但可能受到深度估计模型精确度的影响，可尝试替换成预测深度权重map,让网络自己学习。
2.DUT的infer代码复现，DUT的project也包含了StabNet和DIFRINT的infer代码，sample截图见附录
3.整理视频AI防抖ppt。

下周计划：

本周进展：
1.为后续评估dvs模型效果，查看了公开源码（DIFRINT，FuSta，DVS）的metric代码，发现了一些差异，经过核对后，发现DVS的计算方法更合理一些，后续以此作为参考；以此为基础完成视频指标计算批处理脚本。
2.在3090上跑了RAFT和PWC-Net的模型推理，测试了显存占用和耗时，作为后续DVS光流预测的替换选择；
3.dvs代码的学习和理解（主要是数据对齐部分）

下周计划：
1.配合海洋采集自定义视频数据集，用于后续训练
2.继续熟悉dvs代码
3.分析目前数据对齐的效果，了解ois原理并尝试去除。

本周进展：
1.将dvs模型中的光流feature提取分支去除，光流信息只用于loss的计算和回传，在自定义数据集上测试，发现无光流的模型倾向于将抖动位移较大的维度拉平，平滑轨迹不够真实。
2.尝试搭建flownet2的运行环境，方便后续训练基于小光流模型的dvs，目前还有些小问题，可能显卡版本偏高，cuda不匹配导致。
下周计划：
1.当前dvs的loss设计较为复杂，调研一下其他的无监督训练方法的loss，看是否可以替换。
2.继续尝试ois的去除，验证对齐效果。

本周进展：
1. flownet2的运行环境配置成功，复现了Hybrid的方法（做full-frame防抖），运行时间过长，且在自定义数据集上复现出了文中提到的limit:前景运动导致的虚化伪影。451 454
2.结合之前分享的ppt，调研了比较典型的3D防抖论文，发现利用CV方法预测相机平移，用于优化dvs方案可能是比较靠谱的。
下周计划：
1.重新看一下相机标定的内容，结合论文理解cv方法计算相机平移的公式，尝试复现。

本周进展：
1. 相机路径规划防撞边文献的调研，思路有一定借鉴意义，但是文中只讨论了无旋转的2D平移模型，模型简化使撞边约束可以简单的显式表现在路径规划函数中；实际运用需要考虑旋转和深度信息，无法简单模仿；
2. 从目前dvs输出的warp矩阵计算每一帧的crop_rate（分别对应原图上下左右的四条边），后续设计带撞边约束的路径规划可用于参考；
下周计划：
1. 相机平移
2. 撞边约束探索

本周进展：
1.AI防抖预研工作总结（代码，文档，相关实验结果整理）
2.了解视频插帧的算法原理

下周计划：
1.AI防抖预研总结收尾
2.慢动作项目对齐，跟进项目进度，尽快熟悉和上手

本周进展：
1. 慢动作插帧相关文献阅读了解
2. 150服务器环境调试，目前已可以进行4倍插帧的正常训练。量化训练环境待验证

下周计划：
1. 按照说明跑通4倍插帧的全流程，包括正常训练，量化训练，模型测试和导出，手机端测试
2. 2倍插帧正常训练，熟悉代码模型结构，尝试量化训练

本周进展：
1. 对2倍插帧的代码做了小修改，使其可以按照说明跑通整个正常训练，量化训练，模型测试和导出的流程
2. 配置手机端测试环境，调通手机端测试过程，可以得到正常的输出结果。
下周计划：
1. 查看模型训练过程中的指标变化规律，选择合适的训练策略和参数调节，优化模型性能；
2. 模型训练过程中，调研2倍插帧的算法论文。

本周进展：
1. 2倍插帧模型训练，调节训练策略和参数；当前最好的量化模型在自定义数据集上的psnr为35.42，模型优化还在继续中。
2. 调研了两篇基于光流的插帧文献：Super SloMo和RRIN。相似的思路是都利用refine网络修正光流，并通过预测weight map缓解遮挡效应。
下周计划：
1. 模型优化过程的同时，在手机上测试并查看生成的视频结果，确保不出现明显的伪影或模糊。
2. 评估手机测试的速度和性能。
3. 继续调研插帧相关的算法论文。

本周进展：
1. 2倍插帧模型量化后在手机上做测试，对于原视频中纹理或细节较多的情况，结果会出现模糊和纹理的错位。
2. 在loss中加入perceptual_loss，继续finetune
下周计划：
1. 加入perceptual_loss（脚本默认使用conv5_4），第一轮finetune指标下降，下周继续进行；尝试以文献为参考，以其他层的perceptual_loss为辅助训练
2. 深入调研基于光流和知识蒸馏的方案RIFE，尝试复现，确定是否可以后量化

2022/2/25
1. 2倍插帧模型的训练尝试了perceptual_loss和gram_loss的组合，与工程中已有的vggModel相比，FILM文献使用的vggModel对模型的finetune更友好，各种loss的组合对模型finetune的影响在下周会有初步的结论。
2. 使用RIFE模型对两个比较困难的case（插帧出现模糊和纹理的错位）进行尝试，发现无法很好的解决问题，且基于光流的RIFE方案会有更明显的错位扭曲。
3. 目前2倍插帧模型在K1和L3上测试的平均推理用时分别为25ms和15ms
1. 继续完善2倍插帧模型的finetune工作，根据各种loss的作用确定下一步训练优化方案
2. 熟悉torch->onnx->dlc的量化方法

2022/2/4
1. 2倍插帧模型的训练尝试了perceptual_loss，gram_loss，laplacian_loss的各种组合，目前精度增益不明显，后续优化的路线；1）根据当前的结果，调节loss的权重和lr，继续finetune；2）推测数据集不够，多篇文献使用了Vimeo-90k数据集，尝试增加该数据集 3）不做pixel shuffle或改用YUV格式训练。
2. 熟悉了pytorch版本的训练和量化方法，调整和理清了库中的代码。
1. 2倍插帧模型继续finetune，调节loss和增加数据集；
2. 更改模型（取消pixel shuffle），测试模型大小和手机推理时间